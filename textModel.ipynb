{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef440a79-9e2e-4b78-9eca-f0eaa2b1b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchmetrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "# Transformations\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, image_paths, labels, text_inputs, tokenizer, transform = None, file_path ='/scratch/pyc298/data'): #make sure to update the file_path\n",
    "    self.image_paths = image_paths\n",
    "    self.labels = labels \n",
    "    self.text_inputs = text_inputs\n",
    "    self.transform = transform\n",
    "    self.file_path = file_path\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    #Get the image \n",
    "    image_path = self.image_paths[idx]\n",
    "\n",
    "    #Generate the Full Path\n",
    "    full_path = os.path.join(self.file_path, image_path)\n",
    "    #Open Image from Path\n",
    "    image = Image.open(full_path).convert('L')  # Grayscale\n",
    "    #Transform\n",
    "    if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "    text = self.text_inputs[idx]\n",
    "    #add encoder and vocab things\n",
    "    encoded_dict = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=250,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    # Extract tensors\n",
    "    input_ids = encoded_dict['input_ids'].squeeze(0)\n",
    "    attention_mask = encoded_dict['attention_mask'].squeeze(0)\n",
    "\n",
    "    return {\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long) #convert to tensor of long\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19481335-8cda-49fd-a567-e894fdc7dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(2024)\n",
    "\n",
    "#make this a config\n",
    "path_to_dir = \"/scratch/pyc298/code\"\n",
    "csv_name = \"final_cxr_free_text.csv\"\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(path_to_dir, csv_name ))\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.40, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size = 0.50, random_state = 42)\n",
    "train_data.reset_index(drop = True, inplace = True)\n",
    "valid_data.reset_index(drop = True, inplace = True)\n",
    "test_data.reset_index(drop = True, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9379ec-2c0c-4f10-823b-8f0e0e9b429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_pneumonia\n",
      "0    13059\n",
      "1     1408\n",
      "Name: count, dtype: int64 is_pneumonia\n",
      "0    4391\n",
      "1     432\n",
      "Name: count, dtype: int64 is_pneumonia\n",
      "0    4340\n",
      "1     483\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"is_pneumonia\"].value_counts(),\n",
    "test_data[\"is_pneumonia\"].value_counts(),\n",
    "valid_data[\"is_pneumonia\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acfe916-a61e-4af4-9b62-b6944f92e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14467 4823 4823\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(image_paths = train_data[\"image_path\"], labels = train_data[\"is_pneumonia\"], text_inputs = train_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "test_dataset = CustomDataset(image_paths = test_data[\"image_path\"], labels = test_data[\"is_pneumonia\"], text_inputs = test_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "valid_dataset = CustomDataset(image_paths = valid_data[\"image_path\"], labels = valid_data[\"is_pneumonia\"], text_inputs = valid_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "batch_size = 64\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388d268-f63e-41c9-8acb-d2c799459c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=1e-4, loss_function=nn.BCEWithLogitsLoss()):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # Metrics\n",
    "        self.train_accuracy = torchmetrics.Accuracy(num_classes=num_classes, task='binary')\n",
    "        self.val_accuracy = torchmetrics.Accuracy(num_classes=num_classes, task='binary')\n",
    "        self.test_accuracy = torchmetrics.Accuracy(num_classes=num_classes, task='binary')\n",
    "        self.precision = torchmetrics.Precision(num_classes=num_classes, task='binary')\n",
    "        self.recall = torchmetrics.Recall(num_classes=num_classes, task='binary')\n",
    "        self.f1_score = torchmetrics.F1Score(num_classes=num_classes, task='binary')\n",
    "        self.auroc = torchmetrics.AUROC(num_classes=num_classes, task='binary')\n",
    "        self.confmat = torchmetrics.ConfusionMatrix(num_classes=num_classes, task='binary')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        targets = batch['labels'].unsqueeze(1).float()\n",
    "        outputs = self(batch['image'], batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_accuracy(outputs,targets), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        targets = batch['labels'].unsqueeze(1).float()\n",
    "        outputs = self(batch['image'], batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_accuracy(outputs, targets), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        targets = batch['labels'].unsqueeze(1).float()\n",
    "        outputs = self(batch['image'], batch['input_ids'], batch['attention_mask'])\n",
    "        loss = self.loss_function(outputs, targets)\n",
    "        preds = torch.sigmoid(outputs).round()\n",
    "\n",
    "        self.confmat.update(preds, targets)\n",
    "\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', self.test_accuracy(outputs, targets),on_epoch=True, prog_bar=True)\n",
    "        self.log('precision', self.precision(outputs, targets),on_epoch=True )\n",
    "        self.log('recall', self.recall(outputs, targets),on_epoch=True)\n",
    "        self.log('f1_score', self.f1_score(outputs, targets),on_epoch=True)\n",
    "        self.log('auroc', self.auroc(outputs, targets),on_epoch=True)\n",
    "\n",
    "    def log_confusion_matrix(self, cm):\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_xlabel('Predicted Labels')\n",
    "        ax.set_ylabel('True Labels')\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Log to TensorBoard\n",
    "        self.logger.experiment.add_figure(\"Confusion Matrix\", fig, self.current_epoch)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        cm = self.confmat.compute().cpu().numpy()\n",
    "        self.log_confusion_matrix(cm)\n",
    "        \n",
    "    def set_parameter_requires_grad(self, freeze_it):\n",
    "      # Freeze layers up to the specified block\n",
    "      layers_to_freeze = {\n",
    "          'conv1': self.image_encoder.conv1,\n",
    "          'layer1': self.image_encoder.layer1,\n",
    "          'layer2': self.image_encoder.layer2,\n",
    "          'layer3': self.image_encoder.layer3,\n",
    "          'layer4': self.image_encoder.layer4\n",
    "      }\n",
    "\n",
    "      for name, layer in layers_to_freeze.items():\n",
    "          if name in freeze_it:\n",
    "              for param in layer.parameters():\n",
    "                  param.requires_grad = False\n",
    "          else:\n",
    "              break  # Stop at the first unfrozen layer\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3169828-6985-449e-bfc3-83e4a50cdb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(BaseModel):\n",
    "    def __init__(self, num_classes, learning_rate=1e-4, loss_function = nn.BCEWithLogitsLoss()):\n",
    "        super().__init__(num_classes, learning_rate, loss_function)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #Loss Function\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        # Text encoder setup\n",
    "        self.text_encoder = BertModel.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "        self.text_fc = nn.Linear(768, 256)\n",
    "\n",
    "        # Combined layers\n",
    "        self.final_fc = nn.Linear(256, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = self.text_fc(text_outputs.pooler_output)\n",
    "        return self.final_fc(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5cf5b-e508-4a41-abe7-014e9dd577f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "train_steps_per_epoch = len(train_dataset) // batch_size + (len(train_dataset) % batch_size > 0)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on the 'val_loss'.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    dirpath='model_checkpoints',  # Directory where the checkpoints will be saved\n",
    "    filename='best_model-{epoch:02d}-{val_loss:.2f}',  # Filename with epoch number and validation loss\n",
    "    save_top_k=1,  # Number of best models to save; 1 means save the best model only\n",
    "    mode='min',  # 'min' mode means the 'val_loss' should be minimized\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "class ClearCacheCallback(Callback):\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared GPU cache\")\n",
    "\n",
    "logger = TensorBoardLogger('model_logs', name='bce_loss_Text_Full')\n",
    "# Callbacks\n",
    "clear_cache_callback = ClearCacheCallback()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=3,          \n",
    "    verbose=True,\n",
    "    mode='min'            # 'min' or 'max' (whether the monitored quantity should decrease or increase)\n",
    ")\n",
    "trainer = pl.Trainer(logger=logger, \n",
    "                     log_every_n_steps=50,  # Log at the end of each epoch\n",
    "                     callbacks=[early_stop_callback, clear_cache_callback, checkpoint_callback],\n",
    "                     max_epochs=10, \n",
    "                     devices=1, \n",
    "                     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\") # Automatically choose GPU if available\n",
    "\n",
    "model = TextModel(num_classes=1, loss_function = nn.BCEWithLogitsLoss())\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders =valid_loader)\n",
    "\n",
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55daf1fc-3822-4c12-942d-2e910d62ac8f",
   "metadata": {},
   "source": [
    "# With 75 Subsampled\n",
    "\n",
    "## Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9613df-53da-43ea-a479-e84e64e2369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(2024)\n",
    "\n",
    "#make this a config\n",
    "path_to_dir = \"/scratch/pyc298/code\"\n",
    "csv_name = \"final_cxr_free_text75.csv\"\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(path_to_dir, csv_name ))\n",
    "\n",
    "# Split the dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.40, random_state=42)\n",
    "valid_data, test_data = train_test_split(test_data, test_size = 0.50, random_state = 42)\n",
    "train_data.reset_index(drop = True, inplace = True)\n",
    "valid_data.reset_index(drop = True, inplace = True)\n",
    "test_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(train_data[\"is_pneumonia\"].value_counts(),\n",
    "test_data[\"is_pneumonia\"].value_counts(),\n",
    "valid_data[\"is_pneumonia\"].value_counts())\n",
    "\n",
    "train_dataset = CustomDataset(image_paths = train_data[\"image_path\"], labels = train_data[\"is_pneumonia\"], text_inputs = train_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "test_dataset = CustomDataset(image_paths = test_data[\"image_path\"], labels = test_data[\"is_pneumonia\"], text_inputs = test_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "valid_dataset = CustomDataset(image_paths = valid_data[\"image_path\"], labels = valid_data[\"is_pneumonia\"], text_inputs = valid_data[\"free_text\"], tokenizer = tokenizer, transform = transformations)\n",
    "batch_size = 64\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe7ca2-0baa-43b9-9803-4f525cbc4b5f",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9e695-d8de-4527-8f8a-4c63f012f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning import Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "train_steps_per_epoch = len(train_dataset) // batch_size + (len(train_dataset) % batch_size > 0)\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on the 'val_loss'.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    dirpath='model_checkpoints',  # Directory where the checkpoints will be saved\n",
    "    filename='best_75Text_bce_model-{epoch:02d}-{val_loss:.2f}',  # Filename with epoch number and validation loss\n",
    "    save_top_k=1,  # Number of best models to save; 1 means save the best model only\n",
    "    mode='min',  # 'min' mode means the 'val_loss' should be minimized\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "class ClearCacheCallback(Callback):\n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared GPU cache\")\n",
    "\n",
    "logger = TensorBoardLogger('model_logs', name='bce_loss_Text_75')\n",
    "# Callbacks\n",
    "clear_cache_callback = ClearCacheCallback()\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',    \n",
    "    patience=3,          \n",
    "    verbose=True,\n",
    "    mode='min'            # 'min' or 'max' (whether the monitored quantity should decrease or increase)\n",
    ")\n",
    "trainer = pl.Trainer(logger=logger, \n",
    "                     log_every_n_steps=50,  # Log at the end of each epoch\n",
    "                     callbacks=[early_stop_callback, clear_cache_callback, checkpoint_callback],\n",
    "                     max_epochs=10, \n",
    "                     devices=1, \n",
    "                     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\") # Automatically choose GPU if available\n",
    "\n",
    "model = TextModel(num_classes=1, loss_function = nn.BCEWithLogitsLoss())\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders =valid_loader)\n",
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLMED",
   "language": "python",
   "name": "dlmed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
